{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SP500 Stock Demo — Notebook 03: Train + Register Model\n",
        "\n",
        "- Generate 3‑month ahead target from hourly features\n",
        "- Time-based split and train `XGBRegressor`\n",
        "- Evaluate metrics (RMSE, MAPE, R2)\n",
        "- Register model in Snowflake Model Registry\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0) Imports and session/context\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark.functions import col, lead, avg, sqrt\n",
        "from snowflake.snowpark.functions import abs as sp_abs\n",
        "from snowflake.snowpark.functions import pow as sp_pow\n",
        "from snowflake.snowpark import Window\n",
        "from snowflake.ml.modeling.pipeline import Pipeline\n",
        "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
        "from snowflake.ml.registry import Registry\n",
        "\n",
        "session = get_active_session()\n",
        "session.sql(\"USE WAREHOUSE DEMO_WH_M\").collect()\n",
        "session.sql(\"USE DATABASE SP500_STOCK_DEMO\").collect()\n",
        "session.sql(\"USE SCHEMA DATA\").collect()\n",
        "\n",
        "session.table('PRICE_FEATURES').limit(5).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Build supervised dataset: 3-month ahead target (approx 378 trading hours)\n",
        "hourly = session.table('PRICE_FEATURES')\n",
        "horizon_hours = 378\n",
        "win_order = Window.partition_by('TICKER').order_by(col('TS'))\n",
        "\n",
        "ds = (\n",
        "    hourly\n",
        "    .with_column('FUT_CLOSE', lead(col('CLOSE'), horizon_hours).over(win_order))\n",
        "    .with_column('TARGET_PCT_3M', (col('FUT_CLOSE')/col('CLOSE') - 1))\n",
        "    .drop('FUT_CLOSE')\n",
        "    .filter(col('TARGET_PCT_3M').is_not_null())\n",
        ")\n",
        "\n",
        "ds.limit(5).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Train/test split by time (last 30 days for test)\n",
        "cutoff = session.sql(\"select dateadd('day', -30, max(TS)) as c from PRICE_FEATURES\").collect()[0]['C']\n",
        "train_df = ds.filter(col('TS') < cutoff)\n",
        "test_df = ds.filter(col('TS') >= cutoff)\n",
        "\n",
        "print({'cutoff': cutoff})\n",
        "print('Train rows:', train_df.count())\n",
        "print('Test rows:', test_df.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Define features and train XGBRegressor\n",
        "feature_cols = ['RET_1','SMA_5','SMA_20','VOL_20','RSI_PROXY','VOLUME','CLOSE']\n",
        "label_col = 'TARGET_PCT_3M'\n",
        "output_col = 'PREDICTED_RETURN'\n",
        "\n",
        "# Print dataset sizes and feature count before training\n",
        "print({\n",
        "    'total_rows': ds.count(),\n",
        "    'train_rows': train_df.count(),\n",
        "    'test_rows': test_df.count(),\n",
        "    'num_features': len(feature_cols)\n",
        "})\n",
        "\n",
        "xgb = XGBRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    input_cols=feature_cols,\n",
        "    label_cols=[label_col],\n",
        "    output_cols=[output_col],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "pipe = Pipeline(steps=[('xgb', xgb)])\n",
        "model = pipe.fit(train_df)\n",
        "print('Model trained.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Evaluate metrics (RMSE, MAPE, R2)\n",
        "\n",
        "def compute_metrics(df, y_col: str, yhat_col: str) -> dict:\n",
        "    rmse = df.select(sqrt(avg(sp_pow(col(y_col) - col(yhat_col), 2))).alias('rmse')).collect()[0]['RMSE']\n",
        "    mape = df.select(avg(sp_abs((col(y_col) - col(yhat_col)) / (col(y_col) + 1e-9))).alias('mape')).collect()[0]['MAPE']\n",
        "    mean_y = df.select(avg(col(y_col)).alias('mean_y')).collect()[0]['MEAN_Y']\n",
        "    sse = df.select(avg(sp_pow(col(y_col) - col(yhat_col), 2)).alias('mse')).collect()[0]['MSE']\n",
        "    sst = df.select(avg(sp_pow(col(y_col) - mean_y, 2)).alias('var')).collect()[0]['VAR']\n",
        "    r2 = 1.0 - (sse / sst if sst and sst != 0 else 0.0)\n",
        "    return {'rmse': rmse, 'mape': mape, 'r2': r2}\n",
        "\n",
        "train_pred = model.predict(train_df).select(label_col, output_col)\n",
        "test_pred = model.predict(test_df).select(label_col, output_col)\n",
        "\n",
        "train_metrics = compute_metrics(train_pred, label_col, output_col)\n",
        "test_metrics = compute_metrics(test_pred, label_col, output_col)\n",
        "\n",
        "print({'train': train_metrics, 'test': test_metrics})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Register in Model Registry (enable Snowflake explainability)\n",
        "reg = Registry(session=session, database_name='SP500_STOCK_DEMO', schema_name='DATA')\n",
        "model_name = 'XGB_SP500_RET3M'\n",
        "\n",
        "models_df = reg.show_models()\n",
        "if models_df.empty or model_name not in models_df['name'].to_list():\n",
        "    version = 'V_1'\n",
        "else:\n",
        "    import ast, builtins\n",
        "    max_v = builtins.max([int(v.split('_')[-1]) for v in ast.literal_eval(models_df.loc[models_df['name']==model_name,'versions'].values[0])])\n",
        "    version = f'V_{max_v+1}'\n",
        "\n",
        "# Provide background data to enable SHAP explainability in the Model Registry\n",
        "bg_pd = train_df.select(feature_cols).limit(1000).to_pandas()\n",
        "\n",
        "mv = reg.log_model(\n",
        "    model,\n",
        "    model_name=model_name,\n",
        "    version_name=version,\n",
        "    conda_dependencies=['snowflake-ml-python'],\n",
        "    comment='XGBRegressor predicting 3-month forward return from hourly features',\n",
        "    metrics={\n",
        "        'train_r2': train_metrics['r2'],\n",
        "        'test_r2': test_metrics['r2'],\n",
        "        'train_rmse': train_metrics['rmse'],\n",
        "        'test_rmse': test_metrics['rmse'],\n",
        "        'train_mape': train_metrics['mape'],\n",
        "        'test_mape': test_metrics['mape'],\n",
        "    },\n",
        "    sample_input_data=bg_pd,\n",
        "    options={'relax_version': False, 'enable_explainability': True}\n",
        ")\n",
        "reg.get_model(model_name).default = version\n",
        "\n",
        "print({'model_name': model_name, 'version': version})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Snowflake explainability: compute SHAP-based global feature importance\n",
        "# explain() availability was enabled during logging (cell 5) using background data.\n",
        "from snowflake.snowpark.functions import abs as sp_abs, col as sp_col\n",
        "\n",
        "reg = Registry(session=session, database_name='SP500_STOCK_DEMO', schema_name='DATA')\n",
        "mv_explain = reg.get_model('XGB_SP500_RET3M').default\n",
        "\n",
        "# Take a small sample of inputs (features only) for explanations\n",
        "sample_sp = ds.select(*feature_cols).limit(200)\n",
        "\n",
        "# Run SHAP explain through Model Registry (Snowflake-built explainability)\n",
        "explanations_sp = mv_explain.run(sample_sp, function_name='explain')\n",
        "\n",
        "# Aggregate mean absolute SHAP per feature\n",
        "ex_pd = explanations_sp.to_pandas()\n",
        "cols = list(ex_pd.columns)\n",
        "shap_map = {}\n",
        "for f in feature_cols:\n",
        "    if f in cols:\n",
        "        shap_map[f] = f\n",
        "    elif f\"SHAP_{f}\" in cols:\n",
        "        shap_map[f] = f\"SHAP_{f}\"\n",
        "\n",
        "import pandas as pd\n",
        "imp_rows = []\n",
        "for f, c in shap_map.items():\n",
        "    imp = float(pd.Series(ex_pd[c]).abs().mean())\n",
        "    imp_rows.append({'feature': f, 'mean_abs_shap': imp})\n",
        "imp_df = pd.DataFrame(imp_rows).sort_values('mean_abs_shap', ascending=False)\n",
        "\n",
        "session.create_dataframe(imp_df).write.save_as_table('FEATURE_SHAP_GLOBAL_TOP', mode='overwrite')\n",
        "print(imp_df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Explainability: feature importances (quick view)\n",
        "try:\n",
        "    sk_pipe = model.to_sklearn()\n",
        "    # If using Pipeline, get the xgb step name accordingly\n",
        "    xgb_step = None\n",
        "    for name, step in sk_pipe.named_steps.items():\n",
        "        if 'xgb' in name.lower():\n",
        "            xgb_step = step\n",
        "            break\n",
        "    if xgb_step is not None and hasattr(xgb_step, 'feature_importances_'):\n",
        "        import pandas as pd\n",
        "        fi = pd.DataFrame({\n",
        "            'feature': feature_cols,\n",
        "            'importance': xgb_step.feature_importances_.tolist()[:len(feature_cols)]\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        fi_sp = session.create_dataframe(fi)\n",
        "        fi_sp.write.save_as_table('FEATURE_IMPORTANCE_SP500', mode='overwrite')\n",
        "        print('Saved FEATURE_IMPORTANCE_SP500')\n",
        "    else:\n",
        "        print('Feature importances not available from sklearn object.')\n",
        "except Exception as e:\n",
        "    print('Explainability step skipped:', e)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
