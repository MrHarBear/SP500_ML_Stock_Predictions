{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "### SP500 Stock Demo — Notebook 03: Train and Register Model\n",
    "\n",
    "This notebook trains an XGBoost regressor to predict 3‑month ahead returns from hourly features, logs experiments, and registers the best model.\n",
    "\n",
    "**What you will do**\n",
    "- Build `TARGET_PCT_3M` using `lead()` per ticker over time\n",
    "- Time-based split with safeguards to keep non-empty train/test\n",
    "- Hyperparameter sweep with explicit experiment tracking (params + metrics)\n",
    "- Select best model and evaluate train/test metrics\n",
    "- Register in Snowflake Model Registry and enable explainability\n",
    "\n",
    "**Outputs**\n",
    "- Experiment: `SP500_XGB_RET3M`\n",
    "- Registry model: `XGB_SP500_RET3M` (default set)\n",
    "- Tables: `FEATURE_SHAP_GLOBAL_TOP`, optionally `FEATURE_IMPORTANCE_SP500`\n",
    "\n",
    "**Structure**\n",
    "1) Session + experiment setup\n",
    "2) Create supervised target\n",
    "3) Time-based split\n",
    "4) Train + sweep + log metrics\n",
    "5) Final selection + evaluation\n",
    "6) Registry + explainability\n",
    "7) Optional: list recent runs\n",
    "\n",
    "**Key functions you will see**\n",
    "- `lead(col('CLOSE'), horizon)` creates the future value for the target\n",
    "- `Window.partition_by('TICKER').order_by(col('TS'))` defines per‑ticker ordering\n",
    "- `Pipeline([...]).fit(train_df)` trains on‑snow; `model.predict(df)` scores on‑snow\n",
    "- Metrics: `mean_squared_error`, `mean_absolute_percentage_error`, `r2_score`\n",
    "- Registry: `Registry(...).log_model(...)` and `get_model(name).default = version`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "# 0) Imports and session/context\n",
    "# - Attach to the active Snowflake session used by this notebook\n",
    "# - Set warehouse, database, schema for all subsequent operations\n",
    "# - Initialize Experiment Tracking for logging runs/metrics/params\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import col, lead, avg, sqrt\n",
    "from snowflake.snowpark.functions import abs as sp_abs\n",
    "from snowflake.snowpark.functions import pow as sp_pow\n",
    "from snowflake.snowpark import Window\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.experiment.experiment_tracking import ExperimentTracking\n",
    "\n",
    "session = get_active_session()\n",
    "session.sql(\"USE WAREHOUSE DEMO_WH_M\").collect()\n",
    "session.sql(\"USE DATABASE SP500_STOCK_DEMO\").collect()\n",
    "session.sql(\"USE SCHEMA DATA\").collect()\n",
    "\n",
    "# Experiment Tracking: all runs in this notebook go under this experiment\n",
    "exp = ExperimentTracking(session=session)\n",
    "exp.set_experiment(\"SP500_XGB_RET3M\")\n",
    "\n",
    "# Quick sanity check the feature table is accessible\n",
    "session.table('PRICE_FEATURES').limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983bd19-06e9-4758-bb8b-36a56d180c5d",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.ml import version\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(VERSION[0],VERSION[1],VERSION[2]))\n",
    "print('Snowflake ML version        : {}.{}.{}'.format(version.VERSION[0],version.VERSION[2],version.VERSION[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "name": "cell3"
   },
   "source": [
    "### Step 1 — Session and experiment setup\n",
    "\n",
    "- Set active Snowflake context: warehouse, database, schema\n",
    "- Initialize Experiment Tracking and select experiment `SP500_XGB_RET3M`\n",
    "- Sanity check `PRICE_FEATURES` is accessible\n",
    "\n",
    "Prerequisites:\n",
    "- Notebook 02 created `SP500_STOCK_DEMO.DATA.PRICE_FEATURES`\n",
    "- Warehouse sized for model training (e.g., `DEMO_WH_M`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "### Dataset columns (sample shown above)\n",
    "\n",
    "- `TICKER`: symbol identifier used to partition time windows\n",
    "- `TS`: timestamp of each hourly observation (ordering key)\n",
    "- `CLOSE`: hourly close price used to compute the label\n",
    "- `VOLUME`: hourly traded volume (feature)\n",
    "- `RET_1`: one‑period return proxy used as a feature\n",
    "- `SMA_5`, `SMA_20`: simple moving averages over 5 and 20 periods\n",
    "- `VOL_20`: rolling volatility proxy over 20 periods\n",
    "- `RSI_PROXY`: simple momentum proxy used as a feature\n",
    "- `TARGET_PCT_3M`: label — 3‑month ahead percentage return (target)\n",
    "\n",
    "Key variables (feature intent):\n",
    "RET_1: very short-term momentum/mean reversion clue from the last period’s return.\n",
    "SMA_5, SMA_20: short/medium trend signals; their relationship to price or to each other (e.g., crossovers) can signal trend shifts.\n",
    "VOL_20: rolling dispersion; volatility regimes often change the risk-reward profile and can modulate the meaning of momentum/trend signals.\n",
    "RSI_PROXY: simplified momentum proxy; momentum can persist, especially over short-to-medium horizons.\n",
    "TARGET_PCT_3M: the label we predict, defined as future close divided by current close minus one, aligning the task to forward 3‑month return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "# 1) Build supervised dataset: create 3-month ahead target\n",
    "# - Source features come from PRICE_FEATURES: TICKER, TS, CLOSE, VOLUME, RET_1, SMA_5, SMA_20, VOL_20, RSI_PROXY, ...\n",
    "# - Compute FUT_CLOSE as a 378-hour lead of CLOSE, then TARGET_PCT_3M = FUT_CLOSE/CLOSE - 1\n",
    "# - Drop rows without a future value\n",
    "hourly = session.table('PRICE_FEATURES')  # Base hourly feature table\n",
    "\n",
    "horizon_hours = 378  # ~3 months of trading hours\n",
    "win_order = Window.partition_by('TICKER').order_by(col('TS'))  # Per-ticker chronological order\n",
    "\n",
    "# Build labeled dataset by adding future close and the 3M-ahead return target\n",
    "ds = (\n",
    "    hourly\n",
    "    # FUT_CLOSE: future close price at +horizon within each ticker's time order\n",
    "    .with_column('FUT_CLOSE', lead(col('CLOSE'), horizon_hours).over(win_order))\n",
    "    # TARGET_PCT_3M: percentage return over the horizon\n",
    "    .with_column('TARGET_PCT_3M', (col('FUT_CLOSE')/col('CLOSE') - 1))\n",
    "    # Remove helper column; keep clean schema for downstream steps\n",
    "    .drop('FUT_CLOSE')\n",
    "    # Keep only rows where the future value exists (label known)\n",
    "    .filter(col('TARGET_PCT_3M').is_not_null())\n",
    ")\n",
    "\n",
    "# Peek at a small sample\n",
    "ds.limit(5).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "### Step 2 — Create supervised target\n",
    "\n",
    "- Goal: turn hourly features into a supervised dataset with a 3‑month ahead target per ticker.\n",
    "\n",
    "- Choose a cutoff near dataset end; back off 30/60/90/120 days if needed\n",
    "- Guarantee non-empty train/test while respecting forecasting horizon\n",
    "- Fallback: approximate 80/20 split by time if necessary\n",
    "\n",
    "Outputs:\n",
    "- `train_df`: rows with `TS < cutoff`\n",
    "- `test_df`: rows with `TS >= cutoff`\n",
    "- How it’s built:\n",
    "  - Define per‑ticker chronological window: `Window.partition_by('TICKER').order_by(col('TS'))`\n",
    "  - Set `horizon_hours = 378` (~3 months of trading hours)\n",
    "  - Compute future close: `FUT_CLOSE = lead(CLOSE, horizon_hours)` over the window\n",
    "  - Compute label: `TARGET_PCT_3M = FUT_CLOSE / CLOSE - 1`\n",
    "  - Drop rows where `FUT_CLOSE` is null (label unknown at horizon)\n",
    "\n",
    "- Why this design:\n",
    "  - Long‑horizon target while respecting time order; avoids using any future features\n",
    "  - Lead computed within each `TICKER` prevents cross‑ticker contamination\n",
    "\n",
    "- Outputs:\n",
    "  - `ds`: the original `PRICE_FEATURES` columns plus `TARGET_PCT_3M`\n",
    "  - A small sample is printed (`ds.limit(5).show()`); see the next markdown cell for column‑by‑column notes\n",
    "\n",
    "- Notes/assumptions:\n",
    "  - If trading hours/holidays are irregular, `378` is an approximation; adjust `horizon_hours` as needed\n",
    "  - Expected input columns include: `TICKER`, `TS`, `CLOSE`, `VOLUME`, `RET_1`, `SMA_5`, `SMA_20`, `VOL_20`, `RSI_PROXY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "# 2) Time-based train/test split with safeguards\n",
    "# - Start near the end of the dataset for test split; if empty due to lead horizon, back off in steps\n",
    "# - Fallback: approximate an 80/20 boundary in time\n",
    "from datetime import timedelta\n",
    "from snowflake.snowpark.functions import max as sp_max\n",
    "\n",
    "# Latest timestamp in the labeled dataset; we will back off from here to define a test window\n",
    "max_ts_ds = ds.select(sp_max(col('TS')).alias('mx')).collect()[0]['MX']\n",
    "\n",
    "# Placeholders we will fill once a valid split is found\n",
    "cutoff = None  # the chosen timestamp boundary\n",
    "train_df = None  # rows strictly before the cutoff\n",
    "test_df = None   # rows at or after the cutoff\n",
    "\n",
    "# Progressive backoff: try several look‑back distances to ensure both splits are non‑empty\n",
    "for days in [30, 60, 90, 120]:\n",
    "    try_cutoff = max_ts_ds - timedelta(days=days)  # candidate boundary \"days\" before the end\n",
    "    train_try = ds.filter(col('TS') < try_cutoff)  # chronological training set\n",
    "    test_try = ds.filter(col('TS') >= try_cutoff)  # chronological test set\n",
    "    # Accept the first candidate that yields non‑empty train and test\n",
    "    if train_try.count() > 0 and test_try.count() > 0:\n",
    "        cutoff = try_cutoff\n",
    "        train_df, test_df = train_try, test_try\n",
    "        break\n",
    "\n",
    "# Fallback: if backoff never yielded a valid split, approximate an 80/20 split by time\n",
    "if cutoff is None:\n",
    "    from snowflake.snowpark.functions import min as sp_min\n",
    "    bounds = ds.select(sp_min(col('TS')).alias('mn'), sp_max(col('TS')).alias('mx')).collect()[0]\n",
    "    mn_ts, mx_ts = bounds['MN'], bounds['MX']  # time range of the dataset\n",
    "    boundary = mn_ts + (mx_ts - mn_ts) * 0.8    # 80% of the time span from start\n",
    "    train_df = ds.filter(col('TS') < boundary)\n",
    "    test_df = ds.filter(col('TS') >= boundary)\n",
    "    cutoff = boundary\n",
    "\n",
    "# Quick visibility into the chosen boundary and resulting sizes\n",
    "print({'cutoff': cutoff})\n",
    "print('Train rows:', train_df.count())\n",
    "print('Test rows:', test_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "# 3) Define feature/label/output columns + basic dataset stats (no training here)\n",
    "feature_cols = ['RET_1','SMA_5','SMA_20','VOL_20','RSI_PROXY','VOLUME','CLOSE']\n",
    "label_col = 'TARGET_PCT_3M'\n",
    "output_col = 'PREDICTED_RETURN'\n",
    "\n",
    "print({\n",
    "    'total_rows': ds.count(),\n",
    "    'train_rows': train_df.count(),\n",
    "    'test_rows': test_df.count(),\n",
    "    'num_features': len(feature_cols)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d333431-cd00-4700-9d20-0eef9c951654",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "### Step 3 — Define Metrics\n",
    "\n",
    "- Choose a cutoff near dataset end; back off 30/60/90/120 days if needed\n",
    "- Guarantee non-empty train/test while respecting forecasting horizon\n",
    "- Fallback: approximate 80/20 split by time if necessary\n",
    "\n",
    "Outputs:\n",
    "- `train_df`: rows with `TS < cutoff`\n",
    "- `test_df`: rows with `TS >= cutoff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "# 3) Utilities: metrics wrappers and identifier sanitizer (no compute)\n",
    "# - Compute RMSE/MAPE/R2 using Snowflake ML metrics on Snowpark DataFrames\n",
    "# - Sanitize run names to valid Snowflake identifiers per docs\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import re\n",
    "\n",
    "\n",
    "def compute_metrics_quick(df, y_col: str, yhat_col: str) -> dict:\n",
    "    rmse = mean_squared_error(df=df, y_true_col_names=y_col, y_pred_col_names=yhat_col, squared=False)\n",
    "    mape = mean_absolute_percentage_error(df=df, y_true_col_names=y_col, y_pred_col_names=yhat_col)\n",
    "    r2 = r2_score(df=df, y_true_col_name=y_col, y_pred_col_name=yhat_col)\n",
    "    return {'rmse': rmse, 'mape': mape, 'r2': r2}\n",
    "\n",
    "\n",
    "def sanitize_identifier(name: str, prefix: str = \"RUN_\") -> str:\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9_$]', '_', name)\n",
    "    if not re.match(r'^[A-Za-z_]', cleaned):\n",
    "        cleaned = f\"{prefix}{cleaned}\"\n",
    "    return cleaned[:255]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "### Step 4 — Hyperparameter sweep + experiment logging\n",
    "\n",
    "- Grid: `max_depth ∈ {4,6}`, `learning_rate ∈ {0.1,0.05}`, `n_estimators ∈ {100,200}`\n",
    "- For each config:\n",
    "  - Train `XGBRegressor` pipeline on-snow\n",
    "  - Predict on test set; compute RMSE, MAPE, R2\n",
    "  - Log params (with dataset context) and metrics to experiment\n",
    "- Run names are sanitized to valid SQL identifiers to avoid collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh = str(session.get_current_warehouse()).strip('\"')\n",
    "print(f\"Current warehouse: {wh}\")\n",
    "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n",
    "\n",
    "session.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n",
    "\n",
    "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000021",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "# 3) Define features and train XGBRegressor (+ compact hyperparam sweep with seaborn plot)\n",
    "feature_cols = ['RET_1','SMA_5','SMA_20','VOL_20','RSI_PROXY','VOLUME','CLOSE']\n",
    "label_col = 'TARGET_PCT_3M'\n",
    "output_col = 'PREDICTED_RETURN'\n",
    "\n",
    "# Print dataset sizes and feature count before training\n",
    "print({\n",
    "    'total_rows': ds.count(),\n",
    "    'train_rows': train_df.count(),\n",
    "    'test_rows': test_df.count(),\n",
    "    'num_features': len(feature_cols)\n",
    "})\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import re\n",
    "\n",
    "# Notebook: use Snowflake ML metrics APIs directly (keyword-only args, singular names).\n",
    "# Note: the server-side stored procedure path uses Snowpark-based metrics due to package availability.\n",
    "\n",
    "def compute_metrics_quick(df, y_col: str, yhat_col: str) -> dict:\n",
    "    rmse = mean_squared_error(df=df, y_true_col_names=y_col, y_pred_col_names=yhat_col, squared=False)\n",
    "    mape = mean_absolute_percentage_error(df=df, y_true_col_names=y_col, y_pred_col_names=yhat_col)\n",
    "    r2 = r2_score(df=df, y_true_col_name=y_col, y_pred_col_name=yhat_col)\n",
    "    return {'rmse': rmse, 'mape': mape, 'r2': r2}\n",
    "\n",
    "\n",
    "def sanitize_identifier(name: str, prefix: str = \"RUN_\") -> str:\n",
    "    \"\"\"Convert an arbitrary string to a valid Snowflake SQL identifier.\n",
    "    - Replace invalid characters with underscores\n",
    "    - Ensure it starts with a letter or underscore (else prefix)\n",
    "    - Truncate to 255 characters\n",
    "    See: https://docs.snowflake.com/en/sql-reference/identifiers-syntax\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9_$]', '_', name)\n",
    "    if not re.match(r'^[A-Za-z_]', cleaned):\n",
    "        cleaned = f\"{prefix}{cleaned}\"\n",
    "    return cleaned[:255]\n",
    "\n",
    "# Compact grid similar to reference (learning_rate vs mape; hue=n_estimators; facet by max_depth)\n",
    "max_depth_opts = [4, 6]\n",
    "learning_rate_opts = [0.1, 0.05]\n",
    "n_estimators_opts = [100, 200]\n",
    "\n",
    "rows = []\n",
    "for depth in max_depth_opts:\n",
    "    for lr in learning_rate_opts:\n",
    "        for n in n_estimators_opts:\n",
    "            display_name = f\"xgb_depth={depth}_lr={lr}_n={n}\"\n",
    "            ts = pd.Timestamp.utcnow().strftime('%Y%m%dT%H%M%S')\n",
    "            run_name = sanitize_identifier(f\"{display_name}_{ts}\")\n",
    "            with exp.start_run(run_name):\n",
    "                exp.log_params({\n",
    "                    \"display_name\": display_name,\n",
    "                    \"model\": \"XGBRegressor\",\n",
    "                    \"max_depth\": int(depth),\n",
    "                    \"learning_rate\": float(lr),\n",
    "                    \"n_estimators\": int(n),\n",
    "                    \"subsample\": 0.8,\n",
    "                    \"colsample_bytree\": 0.8,\n",
    "                    \"feature_cols\": feature_cols,\n",
    "                    \"label_col\": label_col,\n",
    "                    \"output_col\": output_col,\n",
    "                    \"time_split_cutoff\": cutoff.isoformat() if cutoff else \"auto\",\n",
    "                    \"dataset_table\": \"PRICE_FEATURES\",\n",
    "                    \"horizon_hours\": int(horizon_hours),\n",
    "                })\n",
    "\n",
    "                xgb_tmp = XGBRegressor(\n",
    "                    n_estimators=n,\n",
    "                    max_depth=depth,\n",
    "                    learning_rate=lr,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    input_cols=feature_cols,\n",
    "                    label_cols=[label_col],\n",
    "                    output_cols=[output_col],\n",
    "                    random_state=42,\n",
    "                )\n",
    "                model_tmp = Pipeline(steps=[('xgb', xgb_tmp)]).fit(train_df)\n",
    "                pred_tmp = model_tmp.predict(test_df).select(label_col, output_col)\n",
    "                m = compute_metrics_quick(pred_tmp, label_col, output_col)\n",
    "                rows.append({\n",
    "                    'max_depth': depth,\n",
    "                    'learning_rate': lr,\n",
    "                    'n_estimators': n,\n",
    "                    'rmse': m['rmse'],\n",
    "                    'mape': m['mape'],\n",
    "                    'r2': m['r2'],\n",
    "                })\n",
    "\n",
    "                exp.log_metrics({\n",
    "                    \"rmse\": float(m[\"rmse\"]),\n",
    "                    \"mape\": float(m[\"mape\"]),\n",
    "                    \"r2\": float(m[\"r2\"]),\n",
    "                })\n",
    "\n",
    "hp_df = pd.DataFrame(rows)\n",
    "# Drop any null metric rows to avoid seaborn errors\n",
    "hp_df = hp_df.dropna(subset=['mape', 'rmse']).sort_values('mape').reset_index(drop=True)\n",
    "\n",
    "# Train final model with best params (by RMSE) if available\n",
    "if not hp_df.empty:\n",
    "    best = hp_df.sort_values('rmse').iloc[0].to_dict()\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=int(best['n_estimators']),\n",
    "        max_depth=int(best['max_depth']),\n",
    "        learning_rate=float(best['learning_rate']),\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        input_cols=feature_cols,\n",
    "        label_cols=[label_col],\n",
    "        output_cols=[output_col],\n",
    "        random_state=42,\n",
    "    )\n",
    "    pipe = Pipeline(steps=[('xgb', xgb)])\n",
    "    model = pipe.fit(train_df)\n",
    "    print('Model trained with best config (by RMSE):', best)\n",
    "else:\n",
    "    # Fall back to baseline params if sweep failed\n",
    "    best = {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.05}\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        input_cols=feature_cols,\n",
    "        label_cols=[label_col],\n",
    "        output_cols=[output_col],\n",
    "        random_state=42,\n",
    "    )\n",
    "    pipe = Pipeline(steps=[('xgb', xgb)])\n",
    "    model = pipe.fit(train_df)\n",
    "    print('Sweep produced no metrics; trained baseline config:', best)\n",
    "\n",
    "# Plot tuning curve similar to reference (learning_rate vs MAPE) if data available\n",
    "if not hp_df.empty:\n",
    "    sns.set_context('notebook', font_scale=0.9)\n",
    "    # Use FacetGrid-like relplot to match style\n",
    "    g = sns.relplot(\n",
    "        data=hp_df,\n",
    "        x='learning_rate', y='mape', hue='n_estimators', col='max_depth',\n",
    "        kind='line', marker='o', height=4, aspect=1.2\n",
    "    )\n",
    "    g.set_titles('max_depth = {col_name}')\n",
    "    g.set_axis_labels('learning_rate', 'MAPE')\n",
    "else:\n",
    "    print('Skipping tuning plot: no sweep results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000020",
   "metadata": {
    "name": "cell21"
   },
   "source": [
    "### Step 4 (cont.) — Consolidate sweep and train best model\n",
    "\n",
    "- Build DataFrame of sweep results and drop null rows\n",
    "- Select best by RMSE and refit full model on `train_df` (fallback to baseline if no results)\n",
    "- Optionally plot tuning curve (learning_rate vs MAPE; hue = `n_estimators`; facet = `max_depth`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000023",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "# 4) Evaluate metrics (RMSE, MAPE, R2) using robust helper\n",
    "train_pred = model.predict(train_df).select(label_col, output_col)\n",
    "test_pred = model.predict(test_df).select(label_col, output_col)\n",
    "\n",
    "train_metrics = compute_metrics_quick(train_pred, label_col, output_col)\n",
    "test_metrics = compute_metrics_quick(test_pred, label_col, output_col)\n",
    "\n",
    "print({'train': train_metrics, 'test': test_metrics})\n",
    "\n",
    "# Log final evaluation and selection as a run for traceability\n",
    "ts_final = pd.Timestamp.utcnow().strftime('%Y%m%dT%H%M%S')\n",
    "with exp.start_run(f\"final_best_selection_{ts_final}\"):\n",
    "    exp.log_params({\n",
    "        \"selected_by\": \"rmse\",\n",
    "        \"best_max_depth\": int(best[\"max_depth\"]) if isinstance(best[\"max_depth\"], (int, float, str)) else best[\"max_depth\"],\n",
    "        \"best_learning_rate\": float(best[\"learning_rate\"]),\n",
    "        \"best_n_estimators\": int(best[\"n_estimators\"]),\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"label_col\": label_col,\n",
    "        \"output_col\": output_col,\n",
    "        \"time_split_cutoff\": cutoff.isoformat() if cutoff else \"auto\",\n",
    "        \"dataset_table\": \"PRICE_FEATURES\",\n",
    "        \"horizon_hours\": int(horizon_hours),\n",
    "    })\n",
    "\n",
    "    exp.log_metrics({\n",
    "        \"train_rmse\": float(train_metrics[\"rmse\"]),\n",
    "        \"train_mape\": float(train_metrics[\"mape\"]),\n",
    "        \"train_r2\": float(train_metrics[\"r2\"]),\n",
    "        \"test_rmse\": float(test_metrics[\"rmse\"]),\n",
    "        \"test_mape\": float(test_metrics[\"mape\"]),\n",
    "        \"test_r2\": float(test_metrics[\"r2\"]),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb570d-e2e5-4f12-b972-ff9844de4c9b",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "# 4c) Diagnostic plots: predicted vs actual, residuals, calibration, and a sample ticker timeseries\n",
    "\n",
    "# Score test set keeping identifiers for plotting\n",
    "scored_sp = model.predict(test_df).select('TICKER', 'TS', label_col, output_col)\n",
    "\n",
    "# Bring a manageable sample to pandas for plotting (adjust if needed)\n",
    "max_rows = 50000\n",
    "scored_pd = scored_sp.limit(max_rows).to_pandas()\n",
    "scored_pd = scored_pd.rename(columns={label_col: 'y_true', output_col: 'y_pred'})\n",
    "scored_pd['residual'] = scored_pd['y_pred'] - scored_pd['y_true']\n",
    "\n",
    "# Basic 2x2 diagnostics\n",
    "sns.set_context('notebook', font_scale=0.9)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 9))\n",
    "\n",
    "# 1) Predicted vs Actual\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(scored_pd['y_true'], scored_pd['y_pred'], s=8, alpha=0.4)\n",
    "lims = [np.nanmin([scored_pd['y_true'], scored_pd['y_pred']]), np.nanmax([scored_pd['y_true'], scored_pd['y_pred']])]\n",
    "ax.plot(lims, lims, 'r--', linewidth=1)\n",
    "ax.set_title('Predicted vs Actual (test)')\n",
    "ax.set_xlabel('Actual (y_true)')\n",
    "ax.set_ylabel('Predicted (y_pred)')\n",
    "\n",
    "# 2) Residuals histogram\n",
    "ax = axes[0, 1]\n",
    "sns.histplot(scored_pd['residual'], bins=40, kde=True, ax=ax)\n",
    "ax.set_title('Residuals distribution (y_pred - y_true)')\n",
    "ax.set_xlabel('Residual')\n",
    "\n",
    "# 3) Residuals vs Fitted\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(scored_pd['y_pred'], scored_pd['residual'], s=8, alpha=0.4)\n",
    "ax.axhline(0.0, color='r', linestyle='--', linewidth=1)\n",
    "ax.set_title('Residuals vs Fitted')\n",
    "ax.set_xlabel('Predicted (y_pred)')\n",
    "ax.set_ylabel('Residual')\n",
    "\n",
    "# 4) Calibration-style: bin by predicted and compare means\n",
    "ax = axes[1, 1]\n",
    "try:\n",
    "    scored_pd['pred_bin'] = pd.qcut(scored_pd['y_pred'], q=10, duplicates='drop')\n",
    "    cal = scored_pd.groupby('pred_bin').agg(mean_pred=('y_pred','mean'), mean_true=('y_true','mean')).reset_index(drop=True)\n",
    "    ax.plot(cal['mean_pred'], cal['mean_true'], marker='o')\n",
    "    lims = [np.nanmin([cal['mean_pred'], cal['mean_true']]), np.nanmax([cal['mean_pred'], cal['mean_true']])]\n",
    "    ax.plot(lims, lims, 'r--', linewidth=1)\n",
    "    ax.set_title('Calibration (bin-mean actual vs predicted)')\n",
    "    ax.set_xlabel('Mean predicted (per bin)')\n",
    "    ax.set_ylabel('Mean actual (per bin)')\n",
    "except Exception:\n",
    "    ax.text(0.5, 0.5, 'Calibration plot unavailable (insufficient variance)', ha='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series for a representative ticker (largest test coverage)\n",
    "if 'TICKER' in scored_pd.columns and 'TS' in scored_pd.columns:\n",
    "    top_ticker = scored_pd['TICKER'].value_counts().index[0]\n",
    "    ts_pd = scored_sp.filter(col('TICKER') == top_ticker).order_by(col('TS')).limit(2000).to_pandas()\n",
    "    ts_pd = ts_pd.rename(columns={label_col: 'y_true', output_col: 'y_pred'})\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(ts_pd['TS'], ts_pd['y_true'], label='Actual', linewidth=1)\n",
    "    plt.plot(ts_pd['TS'], ts_pd['y_pred'], label='Predicted', linewidth=1)\n",
    "    plt.title(f'Actual vs Predicted over time — {top_ticker}')\n",
    "    plt.xlabel('Time (TS)')\n",
    "    plt.ylabel('Return (3M ahead)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Optional: residuals by ticker (top 12 by count)\n",
    "if 'TICKER' in scored_pd.columns:\n",
    "    top_tickers = scored_pd['TICKER'].value_counts().head(12).index\n",
    "    sub = scored_pd[scored_pd['TICKER'].isin(top_tickers)]\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.boxplot(data=sub, x='TICKER', y='residual')\n",
    "    plt.title('Residuals by Ticker (top 12 by test rows)')\n",
    "    plt.xlabel('Ticker')\n",
    "    plt.ylabel('Residual (y_pred - y_true)')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic plots — interpretation and next steps\n",
    "\n",
    "- Predicted vs Actual (test):\n",
    "  - Expectation: tight cloud around 45°; unbiased scale.\n",
    "  - Observed: predictions compressed near 0; underprediction for positive actuals. Indicates weak signal and shrinkage to mean.\n",
    "\n",
    "- Residuals distribution:\n",
    "  - Expectation: centered around 0, roughly symmetric.\n",
    "  - Observed: long negative tail; asymmetric. Systematic underprediction when actuals are large.\n",
    "\n",
    "- Residuals vs Fitted:\n",
    "  - Expectation: no structure; constant variance around 0.\n",
    "  - Observed: banding/heteroskedasticity. Missing regime/ticker structure.\n",
    "\n",
    "- Calibration (bin-mean actual vs predicted):\n",
    "  - Expectation: near 45° line.\n",
    "  - Observed: off-diagonal and irregular. Poor calibration/scale.\n",
    "\n",
    "- Time series (sample ticker):\n",
    "  - Expectation: predicted co-moves with actual and captures direction.\n",
    "  - Observed: predicted nearly flat near 0 while actuals vary substantially.\n",
    "\n",
    "- Residuals by ticker:\n",
    "  - Expectation: medians near 0, similar spread.\n",
    "  - Observed: ticker-specific bias and different spreads.\n",
    "\n",
    "Metrics snapshot (best config):\n",
    "- Train: rmse=0.1653, mape=1.8218, r2=0.0735\n",
    "- Test: rmse=0.1401, mape=2.1695, r2=−0.0714\n",
    "- Takeaway: negative test R² and poor plots imply the model underperforms a constant baseline and is poorly calibrated; predictions are overly shrunk to zero.\n",
    "\n",
    "Actionable improvements:\n",
    "- Features: add market/sector context (SPY/sector returns), multi-horizon momentum (RET_5/RET_20), SMA/EMA ratios, rolling beta, volatility regime (VIX), cross-sectional ranks.\n",
    "- Target: consider log-return, winsorization, or reframing to sign classification/ranking; MAPE is unstable for near-zero targets.\n",
    "- Training: time-series CV, wider hyperparam search with early stopping, longer n_estimators, deeper trees; expand data horizon.\n",
    "- Post-model: revisit calibration after achieving non-negative R².\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "collapsed": false,
    "name": "cell8"
   },
   "source": [
    "### Step 5 — Final selection and evaluation\n",
    "\n",
    "- Re-score train/test using the selected model\n",
    "- Compute and print RMSE, MAPE, R2 for both splits\n",
    "- Log `final_best_selection` run with:\n",
    "  - Selected hyperparameters\n",
    "  - Dataset context (features, label, cutoff, table, horizon)\n",
    "  - Final metrics (`train_*`, `test_*`)\n",
    "\n",
    "Tip: Run names and identifiers are sanitized (letters/digits/`_`/`$`, start with letter/underscore, max 255 chars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000024",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "# 5) Register in Model Registry (enable Snowflake explainability)\n",
    "reg = Registry(session=session, database_name='SP500_STOCK_DEMO', schema_name='DATA')\n",
    "model_name = 'XGB_SP500_RET3M'\n",
    "\n",
    "models_df = reg.show_models()\n",
    "if models_df.empty or model_name not in models_df['name'].to_list():\n",
    "    version = 'V_1'\n",
    "else:\n",
    "    import ast, builtins\n",
    "    max_v = builtins.max([int(v.split('_')[-1]) for v in ast.literal_eval(models_df.loc[models_df['name']==model_name,'versions'].values[0])])\n",
    "    version = f'V_{max_v+1}'\n",
    "\n",
    "# Provide background data to enable SHAP explainability in the Model Registry\n",
    "bg_pd = train_df.select(feature_cols).limit(1000).to_pandas()\n",
    "\n",
    "mv = reg.log_model(\n",
    "    model,\n",
    "    model_name=model_name,\n",
    "    version_name=version,\n",
    "    conda_dependencies=['snowflake-ml-python'],\n",
    "    comment='XGBRegressor predicting 3-month forward return from hourly features',\n",
    "    metrics={\n",
    "        'train_r2': train_metrics['r2'],\n",
    "        'test_r2': test_metrics['r2'],\n",
    "        'train_rmse': train_metrics['rmse'],\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'train_mape': train_metrics['mape'],\n",
    "        'test_mape': test_metrics['mape'],\n",
    "    },\n",
    "    sample_input_data=bg_pd,\n",
    "    options={'relax_version': False, 'enable_explainability': True}\n",
    ")\n",
    "reg.get_model(model_name).default = version\n",
    "\n",
    "# Log registry pointers into the final run context\n",
    "ts_reg = pd.Timestamp.utcnow().strftime('%Y%m%dT%H%M%S')\n",
    "with exp.start_run(f\"final_best_selection_regref_{ts_reg}\"):\n",
    "    exp.log_params({\n",
    "        \"registry_model_name\": model_name,\n",
    "        \"registry_version\": version,\n",
    "    })\n",
    "\n",
    "print({'model_name': model_name, 'version': version})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "name": "cell9"
   },
   "source": [
    "### Step 6 — Model Registry and experiment linkage\n",
    "\n",
    "- Register model in Snowflake Model Registry and set default version\n",
    "- Provide background sample to enable explainability (`explain()`): ~1000 training rows of features\n",
    "- Log registry pointers back to the experiment (`registry_model_name`, `registry_version`) for traceability\n",
    "\n",
    "Versioning:\n",
    "- If the model exists, increment numeric suffix (e.g., `V_2`, `V_3`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000025",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "# 6) Snowflake explainability: compute SHAP-based global feature importance + seaborn bar plot\n",
    "# explain() availability was enabled during logging (cell 5) using background data.\n",
    "from snowflake.snowpark.functions import abs as sp_abs, col as sp_col\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reg = Registry(session=session, database_name='SP500_STOCK_DEMO', schema_name='DATA')\n",
    "mv_explain = reg.get_model('XGB_SP500_RET3M').default\n",
    "\n",
    "# Take a small sample of inputs (features only) for explanations\n",
    "sample_sp = ds.select(*feature_cols).limit(200)\n",
    "\n",
    "# Run SHAP explain through Model Registry (Snowflake-built explainability)\n",
    "explanations_sp = mv_explain.run(sample_sp, function_name='explain')\n",
    "\n",
    "# Aggregate mean absolute SHAP per feature\n",
    "ex_pd = explanations_sp.to_pandas()\n",
    "cols = list(ex_pd.columns)\n",
    "shap_map = {}\n",
    "for f in feature_cols:\n",
    "    if f in cols:\n",
    "        shap_map[f] = f\n",
    "    elif f\"SHAP_{f}\" in cols:\n",
    "        shap_map[f] = f\"SHAP_{f}\"\n",
    "\n",
    "imp_rows = []\n",
    "for f, c in shap_map.items():\n",
    "    imp = float(pd.Series(ex_pd[c]).abs().mean())\n",
    "    imp_rows.append({'feature': f, 'mean_abs_shap': imp})\n",
    "imp_df = pd.DataFrame(imp_rows).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "# Persist and plot\n",
    "session.create_dataframe(imp_df).write.save_as_table('FEATURE_SHAP_GLOBAL_TOP', mode='overwrite')\n",
    "\n",
    "sns.set_context('notebook', font_scale=0.9)\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=imp_df, x='mean_abs_shap', y='feature', orient='h')\n",
    "plt.title('Global feature importance (mean |SHAP|)')\n",
    "plt.xlabel('Mean |SHAP|')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(imp_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000026",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "# 6b) Explainability: feature importances (from sklearn view) — optional quick view\n",
    "sk_pipe = model.to_sklearn()\n",
    "# Find xgb step\n",
    "xgb_step = None\n",
    "for name, step in sk_pipe.named_steps.items():\n",
    "    if 'xgb' in name.lower():\n",
    "        xgb_step = step\n",
    "        break\n",
    "\n",
    "if xgb_step is not None and hasattr(xgb_step, 'feature_importances_'):\n",
    "    import pandas as pd\n",
    "    fi = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': xgb_step.feature_importances_.tolist()[:len(feature_cols)]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    fi_sp = session.create_dataframe(fi)\n",
    "    fi_sp.write.save_as_table('FEATURE_IMPORTANCE_SP500', mode='overwrite')\n",
    "    print('Saved FEATURE_IMPORTANCE_SP500')\n",
    "else:\n",
    "    print('Feature importances not available from sklearn object.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh = str(session.get_current_warehouse()).strip('\"')\n",
    "print(f\"Current warehouse: {wh}\")\n",
    "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n",
    "\n",
    "session.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = SMALL WAIT_FOR_COMPLETION = TRUE\").collect()\n",
    "\n",
    "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "authorEmail": "harley.chen@snowflake.com",
   "authorId": "167081822753",
   "authorName": "HCHEN",
   "lastEditTime": 1755088493476,
   "notebookId": "il7joixr66bl5vqwgu2e",
   "sessionId": "000bb193-bbe7-45cb-9746-13a20c2dbf1b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
