{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SP500 Stock Demo — Notebook 03: Train + Register Model\n",
        "\n",
        "- Generate 3‑month ahead target from hourly features\n",
        "- Time-based split and train `XGBRegressor`\n",
        "- Evaluate metrics (RMSE, MAPE, R2)\n",
        "- Register model in Snowflake Model Registry\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What this notebook does\n",
        "\n",
        "- Build a supervised dataset from `PRICE_FEATURES` by creating a 3‑month ahead target (`TARGET_PCT_3M`) using `lead()` over time windows per ticker.\n",
        "- Perform a time‑based train/test split that guards against empty test sets due to the forecasting horizon.\n",
        "- Train an `XGBRegressor` with a compact hyperparameter sweep; compute metrics with Snowflake ML metrics on Snowpark DataFrames.\n",
        "- Select the best configuration, evaluate final metrics, and register the model in the Snowflake Model Registry with explainability enabled.\n",
        "- Log all key steps with Snowflake Experiment Tracking to enable comparing runs in Snowsight.\n",
        "\n",
        "### Experiment Tracking: explicit logging (no autologging/artifacts yet)\n",
        "- This release does not support autologging or artifact logging.\n",
        "- We explicitly call:\n",
        "  - `exp.start_run(<name>)` to start a run for each sweep trial and for the final selection.\n",
        "  - `exp.log_params({...})` to record hyperparameters and dataset context (features, label, cutoff, table, horizon).\n",
        "  - `exp.log_metrics({...})` to record evaluation metrics (RMSE, MAPE, R2 for test/train).\n",
        "  - A small run after registry logging to record `registry_model_name` and `registry_version` so runs and registry stay connected.\n",
        "\n",
        "### Key functions you will see\n",
        "- `lead(col('CLOSE'), horizon)` creates the future value for the target.\n",
        "- `Window.partition_by('TICKER').order_by(col('TS'))` defines a per‑ticker ordering for time‑based ops.\n",
        "- `Pipeline([...]).fit(train_df)` trains on‑snow; `model.predict(df)` scores on‑snow.\n",
        "- Metrics: `mean_squared_error(..., squared=False)`, `mean_absolute_percentage_error(...)`, `r2_score(...)` over Snowpark DataFrames.\n",
        "- Registry: `Registry(...).log_model(...)` and `get_model(name).default = version`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Session and Experiment initialization\n",
        "\n",
        "- We attach to the active Snowflake session used by the notebook and set the warehouse, database, and schema.\n",
        "- We initialize the Experiment Tracking client with `ExperimentTracking(session)` and select the experiment `SP500_XGB_RET3M`.\n",
        "- All subsequent `start_run`, `log_params`, and `log_metrics` calls are recorded under this experiment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating the supervised learning target\n",
        "\n",
        "- `lead(CLOSE, horizon_hours)` shifts close prices forward within each ticker partition to fetch the future price.\n",
        "- `TARGET_PCT_3M = FUT_CLOSE / CLOSE - 1` is the prediction target: the 3‑month ahead percentage return.\n",
        "- Rows without a future close are dropped, ensuring we only train where the target is known.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Time-based split with safeguards\n",
        "\n",
        "- We choose a cutoff near the end of the dataset for testing but back off (30/60/90/120 days) if the forecast horizon would empty the test set.\n",
        "- If that still fails, we approximate an 80/20 split over time.\n",
        "- This yields non-empty `train_df` and `test_df` aligned to a realistic forecasting scenario.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter sweep and explicit run logging\n",
        "\n",
        "- Grid over `max_depth`, `learning_rate`, and `n_estimators`.\n",
        "- For each combo we:\n",
        "  - Create an `XGBRegressor` pipeline, `fit(train_df)`, `predict(test_df)`.\n",
        "  - Compute metrics (`rmse`, `mape`, `r2`) using Snowflake ML metrics.\n",
        "  - Start an Experiment run: `with exp.start_run(f\"xgb_depth={depth}_lr={lr}_n={n}\")`.\n",
        "  - Explicitly log hyperparameters and dataset context: `exp.log_params({...})`.\n",
        "  - Explicitly log metrics: `exp.log_metrics({...})`.\n",
        "- We consolidate results into a DataFrame and visualize a compact tuning curve.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0) Imports and session/context\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark.functions import col, lead, avg, sqrt\n",
        "from snowflake.snowpark.functions import abs as sp_abs\n",
        "from snowflake.snowpark.functions import pow as sp_pow\n",
        "from snowflake.snowpark import Window\n",
        "from snowflake.ml.modeling.pipeline import Pipeline\n",
        "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
        "from snowflake.ml.registry import Registry\n",
        "from snowflake.ml.experiment.experiment_tracking import ExperimentTracking\n",
        "\n",
        "session = get_active_session()\n",
        "session.sql(\"USE WAREHOUSE DEMO_WH_M\").collect()\n",
        "session.sql(\"USE DATABASE SP500_STOCK_DEMO\").collect()\n",
        "session.sql(\"USE SCHEMA DATA\").collect()\n",
        "\n",
        "# Experiment Tracking\n",
        "exp = ExperimentTracking(session=session)\n",
        "exp.set_experiment(\"SP500_XGB_RET3M\")\n",
        "\n",
        "# Ensure we are using the V2 feature table with SECTOR enrichment\n",
        "session.table('PRICE_FEATURES').limit(5).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final model selection, evaluation, and run logging\n",
        "\n",
        "- Select the best configuration by RMSE (or fall back to a baseline) and refit.\n",
        "- Evaluate train/test metrics and print them for quick inspection.\n",
        "- Start a final selection run `with exp.start_run(\"final_best_selection\")` and explicitly:\n",
        "  - `log_params`: chosen hyperparameters and dataset context.\n",
        "  - `log_metrics`: `train_rmse`, `train_mape`, `train_r2`, `test_rmse`, `test_mape`, `test_r2`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Registry and experiment linkage\n",
        "\n",
        "- Register the model and set the default version in the Snowflake Model Registry.\n",
        "- Because artifact logging is not yet supported, we record the registry identifiers back into the experiment via a short run `final_best_selection_regref` with `exp.log_params({\"registry_model_name\":..., \"registry_version\":...})`.\n",
        "- This makes it easy in Snowsight to trace from experiments to the deployed model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Build supervised dataset: 3-month ahead target (approx 378 trading hours)\n",
        "hourly = session.table('PRICE_FEATURES')\n",
        "horizon_hours = 378\n",
        "win_order = Window.partition_by('TICKER').order_by(col('TS'))\n",
        "\n",
        "ds = (\n",
        "    hourly\n",
        "    .with_column('FUT_CLOSE', lead(col('CLOSE'), horizon_hours).over(win_order))\n",
        "    .with_column('TARGET_PCT_3M', (col('FUT_CLOSE')/col('CLOSE') - 1))\n",
        "    .drop('FUT_CLOSE')\n",
        "    .filter(col('TARGET_PCT_3M').is_not_null())\n",
        ")\n",
        "\n",
        "ds.limit(5).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Train/test split by time (choose cutoff from supervised ds; fallback to ensure non-empty test)\n",
        "from datetime import timedelta\n",
        "from snowflake.snowpark.functions import max as sp_max\n",
        "\n",
        "max_ts_ds = ds.select(sp_max(col('TS')).alias('mx')).collect()[0]['MX']\n",
        "cutoff = None\n",
        "train_df = None\n",
        "test_df = None\n",
        "\n",
        "# Try progressively older cutoffs to avoid empty test set due to lead horizon trimming\n",
        "for days in [30, 60, 90, 120]:\n",
        "    try_cutoff = max_ts_ds - timedelta(days=days)\n",
        "    train_try = ds.filter(col('TS') < try_cutoff)\n",
        "    test_try = ds.filter(col('TS') >= try_cutoff)\n",
        "    trc = train_try.count()\n",
        "    tec = test_try.count()\n",
        "    if trc > 0 and tec > 0:\n",
        "        cutoff = try_cutoff\n",
        "        train_df, test_df = train_try, test_try\n",
        "        break\n",
        "\n",
        "# As a last resort, split by 80/20 time percentile if above failed\n",
        "if cutoff is None:\n",
        "    # Approximate by taking a coarse boundary in timestamp space\n",
        "    # Use min/max from ds\n",
        "    from snowflake.snowpark.functions import min as sp_min\n",
        "    bounds = ds.select(sp_min(col('TS')).alias('mn'), sp_max(col('TS')).alias('mx')).collect()[0]\n",
        "    mn_ts, mx_ts = bounds['MN'], bounds['MX']\n",
        "    boundary = mn_ts + (mx_ts - mn_ts) * 0.8\n",
        "    train_df = ds.filter(col('TS') < boundary)\n",
        "    test_df = ds.filter(col('TS') >= boundary)\n",
        "    cutoff = boundary\n",
        "\n",
        "print({'cutoff': cutoff})\n",
        "print('Train rows:', train_df.count())\n",
        "print('Test rows:', test_df.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Define features and basic names used downstream\n",
        "feature_cols = ['RET_1','SMA_5','SMA_20','VOL_20','RSI_PROXY','VOLUME','CLOSE']\n",
        "label_col = 'TARGET_PCT_3M'\n",
        "output_col = 'PREDICTED_RETURN'\n",
        "\n",
        "print({\n",
        "    'total_rows': ds.count(),\n",
        "    'train_rows': train_df.count(),\n",
        "    'test_rows': test_df.count(),\n",
        "    'num_features': len(feature_cols)\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics utilities and identifier sanitizer (compute-free)\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
        "import re\n",
        "\n",
        "\n",
        "def compute_metrics_quick(df, y_col: str, yhat_col: str) -> dict:\n",
        "    rmse = mean_squared_error(df=df, y_true_col_names=y_col, y_pred_col_names=yhat_col, squared=False)\n",
        "    mape = mean_absolute_percentage_error(df=df, y_true_col_names=y_col, y_pred_col_names=yhat_col)\n",
        "    r2 = r2_score(df=df, y_true_col_name=y_col, y_pred_col_name=yhat_col)\n",
        "    return {'rmse': rmse, 'mape': mape, 'r2': r2}\n",
        "\n",
        "\n",
        "def sanitize_identifier(name: str, prefix: str = \"RUN_\") -> str:\n",
        "    cleaned = re.sub(r'[^A-Za-z0-9_$]', '_', name)\n",
        "    if not re.match(r'^[A-Za-z_]', cleaned):\n",
        "        cleaned = f\"{prefix}{cleaned}\"\n",
        "    return cleaned[:255]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Precompute identifier-safe run names (no model training here)\n",
        "param_grid = []\n",
        "for depth in [4, 6]:\n",
        "    for lr in [0.1, 0.05]:\n",
        "        for n in [100, 200]:\n",
        "            display_name = f\"xgb_depth={depth}_lr={lr}_n={n}\"\n",
        "            run_name = sanitize_identifier(display_name)\n",
        "            param_grid.append({\n",
        "                \"depth\": depth,\n",
        "                \"lr\": lr,\n",
        "                \"n\": n,\n",
        "                \"display_name\": display_name,\n",
        "                \"run_name\": run_name,\n",
        "            })\n",
        "\n",
        "# Quick check: show mapping of display_name -> run_name\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(pd.DataFrame(param_grid))\n",
        "except Exception:\n",
        "    print(param_grid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3a) Train/eval with precomputed names (minimizes duplicate compute)\n",
        "rows = []\n",
        "for cfg in param_grid:\n",
        "    depth, lr, n = cfg[\"depth\"], cfg[\"lr\"], cfg[\"n\"]\n",
        "    display_name, base_name = cfg[\"display_name\"], cfg[\"run_name\"]\n",
        "\n",
        "    # Ensure unique run name by suffixing _V2, _V3, ... if needed\n",
        "    suffix = 1\n",
        "    while True:\n",
        "        candidate = base_name if suffix == 1 else f\"{base_name}_V{suffix}\"\n",
        "        try:\n",
        "            with exp.start_run(candidate):\n",
        "                exp.log_params({\n",
        "                    \"display_name\": display_name,\n",
        "                    \"run_name_variant\": candidate,\n",
        "                    \"model\": \"XGBRegressor\",\n",
        "                    \"max_depth\": int(depth),\n",
        "                    \"learning_rate\": float(lr),\n",
        "                    \"n_estimators\": int(n),\n",
        "                    \"subsample\": 0.8,\n",
        "                    \"colsample_bytree\": 0.8,\n",
        "                    \"feature_cols\": feature_cols,\n",
        "                    \"label_col\": label_col,\n",
        "                    \"output_col\": output_col,\n",
        "                    \"time_split_cutoff\": cutoff.isoformat() if cutoff else \"auto\",\n",
        "                    \"dataset_table\": \"PRICE_FEATURES\",\n",
        "                    \"horizon_hours\": int(horizon_hours),\n",
        "                })\n",
        "\n",
        "                xgb_tmp = XGBRegressor(\n",
        "                    n_estimators=n,\n",
        "                    max_depth=depth,\n",
        "                    learning_rate=lr,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    input_cols=feature_cols,\n",
        "                    label_cols=[label_col],\n",
        "                    output_cols=[output_col],\n",
        "                    random_state=42,\n",
        "                )\n",
        "                model_tmp = Pipeline(steps=[('xgb', xgb_tmp)]).fit(train_df)\n",
        "                pred_tmp = model_tmp.predict(test_df).select(label_col, output_col)\n",
        "                m = compute_metrics_quick(pred_tmp, label_col, output_col)\n",
        "                rows.append({\n",
        "                    'max_depth': depth,\n",
        "                    'learning_rate': lr,\n",
        "                    'n_estimators': n,\n",
        "                    'rmse': m['rmse'],\n",
        "                    'mape': m['mape'],\n",
        "                    'r2': m['r2'],\n",
        "                })\n",
        "\n",
        "                exp.log_metrics({\n",
        "                    \"rmse\": float(m[\"rmse\"]),\n",
        "                    \"mape\": float(m[\"mape\"]),\n",
        "                    \"r2\": float(m[\"r2\"]),\n",
        "                })\n",
        "            break\n",
        "        except Exception as e:\n",
        "            msg = str(e)\n",
        "            if \"already exists\" in msg and \"Experiment\" in msg:\n",
        "                suffix += 1\n",
        "                continue\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "hp_df = pd.DataFrame(rows)\n",
        "hp_df = hp_df.dropna(subset=['mape', 'rmse']).sort_values('mape').reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Define features and basic names used downstream\n",
        "feature_cols = ['RET_1','SMA_5','SMA_20','VOL_20','RSI_PROXY','VOLUME','CLOSE']\n",
        "label_col = 'TARGET_PCT_3M'\n",
        "output_col = 'PREDICTED_RETURN'\n",
        "\n",
        "print({\n",
        "    'total_rows': ds.count(),\n",
        "    'train_rows': train_df.count(),\n",
        "    'test_rows': test_df.count(),\n",
        "    'num_features': len(feature_cols)\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics utilities and identifier sanitizer (compute-free)\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
        "import re\n",
        "\n",
        "\n",
        "def compute_metrics_quick(df, y_col: str, yhat_col: str) -> dict:\n",
        "    rmse = mean_squared_error(df=df, y_true_col_names=y_col, y_pred_col_names=yhat_col, squared=False)\n",
        "    mape = mean_absolute_percentage_error(df=df, y_true_col_names=y_col, y_pred_col_names=yhat_col)\n",
        "    r2 = r2_score(df=df, y_true_col_name=y_col, y_pred_col_name=yhat_col)\n",
        "    return {'rmse': rmse, 'mape': mape, 'r2': r2}\n",
        "\n",
        "\n",
        "def sanitize_identifier(name: str, prefix: str = \"RUN_\") -> str:\n",
        "    cleaned = re.sub(r'[^A-Za-z0-9_$]', '_', name)\n",
        "    if not re.match(r'^[A-Za-z_]', cleaned):\n",
        "        cleaned = f\"{prefix}{cleaned}\"\n",
        "    return cleaned[:255]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Identifier-safe run names and debugging helpers\n",
        "\n",
        "To avoid SQL identifier errors for run names, we validate/sanitize names before training. Snowflake identifier rules: start with a letter/underscore; only letters, digits, `_`, `$`; max 255 chars. See: [Identifier requirements](https://docs.snowflake.com/en/sql-reference/identifiers-syntax).\n",
        "\n",
        "The next cell performs a fast, compute-free validation of the run names derived from hyperparameters and builds a mapping we reuse in the sweep to avoid recomputing and to ensure consistency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Precompute identifier-safe run names (no model training here)\n",
        "param_grid = []\n",
        "for depth in [4, 6]:\n",
        "    for lr in [0.1, 0.05]:\n",
        "        for n in [100, 200]:\n",
        "            display_name = f\"xgb_depth={depth}_lr={lr}_n={n}\"\n",
        "            run_name = sanitize_identifier(display_name)\n",
        "            param_grid.append({\n",
        "                \"depth\": depth,\n",
        "                \"lr\": lr,\n",
        "                \"n\": n,\n",
        "                \"display_name\": display_name,\n",
        "                \"run_name\": run_name,\n",
        "            })\n",
        "\n",
        "# Quick check: show mapping of display_name -> run_name\n",
        "try:\n",
        "    import pandas as pd\n",
        "    from IPython.display import display\n",
        "    display(pd.DataFrame(param_grid))\n",
        "except Exception:\n",
        "    print(param_grid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3a) Train/eval with precomputed names (minimizes duplicate compute)\n",
        "rows = []\n",
        "for cfg in param_grid:\n",
        "    depth, lr, n = cfg[\"depth\"], cfg[\"lr\"], cfg[\"n\"]]\n",
        "    display_name, run_name = cfg[\"display_name\"], cfg[\"run_name\"]\n",
        "    with exp.start_run(run_name):\n",
        "        exp.log_params({\n",
        "            \"display_name\": display_name,\n",
        "            \"model\": \"XGBRegressor\",\n",
        "            \"max_depth\": int(depth),\n",
        "            \"learning_rate\": float(lr),\n",
        "            \"n_estimators\": int(n),\n",
        "            \"subsample\": 0.8,\n",
        "            \"colsample_bytree\": 0.8,\n",
        "            \"feature_cols\": feature_cols,\n",
        "            \"label_col\": label_col,\n",
        "            \"output_col\": output_col,\n",
        "            \"time_split_cutoff\": cutoff.isoformat() if cutoff else \"auto\",\n",
        "            \"dataset_table\": \"PRICE_FEATURES\",\n",
        "            \"horizon_hours\": int(horizon_hours),\n",
        "        })\n",
        "\n",
        "        xgb_tmp = XGBRegressor(\n",
        "            n_estimators=n,\n",
        "            max_depth=depth,\n",
        "            learning_rate=lr,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            input_cols=feature_cols,\n",
        "            label_cols=[label_col],\n",
        "            output_cols=[output_col],\n",
        "            random_state=42,\n",
        "        )\n",
        "        model_tmp = Pipeline(steps=[('xgb', xgb_tmp)]).fit(train_df)\n",
        "        pred_tmp = model_tmp.predict(test_df).select(label_col, output_col)\n",
        "        m = compute_metrics_quick(pred_tmp, label_col, output_col)\n",
        "        rows.append({\n",
        "            'max_depth': depth,\n",
        "            'learning_rate': lr,\n",
        "            'n_estimators': n,\n",
        "            'rmse': m['rmse'],\n",
        "            'mape': m['mape'],\n",
        "            'r2': m['r2'],\n",
        "        })\n",
        "\n",
        "        exp.log_metrics({\n",
        "            \"rmse\": float(m[\"rmse\"]),\n",
        "            \"mape\": float(m[\"mape\"]),\n",
        "            \"r2\": float(m[\"r2\"]),\n",
        "        })\n",
        "\n",
        "hp_df = pd.DataFrame(rows)\n",
        "hp_df = hp_df.dropna(subset=['mape', 'rmse']).sort_values('mape').reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Define features and train XGBRegressor (+ compact hyperparam sweep with seaborn plot)\n",
        "feature_cols = ['RET_1','SMA_5','SMA_20','VOL_20','RSI_PROXY','VOLUME','CLOSE']\n",
        "label_col = 'TARGET_PCT_3M'\n",
        "output_col = 'PREDICTED_RETURN'\n",
        "\n",
        "# Print dataset sizes and feature count before training\n",
        "print({\n",
        "    'total_rows': ds.count(),\n",
        "    'train_rows': train_df.count(),\n",
        "    'test_rows': test_df.count(),\n",
        "    'num_features': len(feature_cols)\n",
        "})\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
        "import re\n",
        "\n",
        "# Notebook: use Snowflake ML metrics APIs directly (keyword-only args, singular names).\n",
        "# Note: the server-side stored procedure path uses Snowpark-based metrics due to package availability.\n",
        "\n",
        "def compute_metrics_quick(df, y_col: str, yhat_col: str) -> dict:\n",
        "    rmse = mean_squared_error(df=df, y_true_col_names=y_col, y_pred_col_names=yhat_col, squared=False)\n",
        "    mape = mean_absolute_percentage_error(df=df, y_true_col_names=y_col, y_pred_col_names=yhat_col)\n",
        "    r2 = r2_score(df=df, y_true_col_name=y_col, y_pred_col_name=yhat_col)\n",
        "    return {'rmse': rmse, 'mape': mape, 'r2': r2}\n",
        "\n",
        "\n",
        "def sanitize_identifier(name: str, prefix: str = \"RUN_\") -> str:\n",
        "    \"\"\"Convert an arbitrary string to a valid Snowflake SQL identifier.\n",
        "    - Replace invalid characters with underscores\n",
        "    - Ensure it starts with a letter or underscore (else prefix)\n",
        "    - Truncate to 255 characters\n",
        "    See: https://docs.snowflake.com/en/sql-reference/identifiers-syntax\n",
        "    \"\"\"\n",
        "    cleaned = re.sub(r'[^A-Za-z0-9_$]', '_', name)\n",
        "    if not re.match(r'^[A-Za-z_]', cleaned):\n",
        "        cleaned = f\"{prefix}{cleaned}\"\n",
        "    return cleaned[:255]\n",
        "\n",
        "# Compact grid similar to reference (learning_rate vs mape; hue=n_estimators; facet by max_depth)\n",
        "max_depth_opts = [4, 6]\n",
        "learning_rate_opts = [0.1, 0.05]\n",
        "n_estimators_opts = [100, 200]\n",
        "\n",
        "rows = []\n",
        "for depth in max_depth_opts:\n",
        "    for lr in learning_rate_opts:\n",
        "        for n in n_estimators_opts:\n",
        "            display_name = f\"xgb_depth={depth}_lr={lr}_n={n}\"\n",
        "            run_name = sanitize_identifier(display_name)\n",
        "            with exp.start_run(run_name):\n",
        "                exp.log_params({\n",
        "                    \"display_name\": display_name,\n",
        "                    \"model\": \"XGBRegressor\",\n",
        "                    \"max_depth\": int(depth),\n",
        "                    \"learning_rate\": float(lr),\n",
        "                    \"n_estimators\": int(n),\n",
        "                    \"subsample\": 0.8,\n",
        "                    \"colsample_bytree\": 0.8,\n",
        "                    \"feature_cols\": feature_cols,\n",
        "                    \"label_col\": label_col,\n",
        "                    \"output_col\": output_col,\n",
        "                    \"time_split_cutoff\": cutoff.isoformat() if cutoff else \"auto\",\n",
        "                    \"dataset_table\": \"PRICE_FEATURES\",\n",
        "                    \"horizon_hours\": int(horizon_hours),\n",
        "                })\n",
        "\n",
        "                xgb_tmp = XGBRegressor(\n",
        "                    n_estimators=n,\n",
        "                    max_depth=depth,\n",
        "                    learning_rate=lr,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    input_cols=feature_cols,\n",
        "                    label_cols=[label_col],\n",
        "                    output_cols=[output_col],\n",
        "                    random_state=42,\n",
        "                )\n",
        "                model_tmp = Pipeline(steps=[('xgb', xgb_tmp)]).fit(train_df)\n",
        "                pred_tmp = model_tmp.predict(test_df).select(label_col, output_col)\n",
        "                m = compute_metrics_quick(pred_tmp, label_col, output_col)\n",
        "                rows.append({\n",
        "                    'max_depth': depth,\n",
        "                    'learning_rate': lr,\n",
        "                    'n_estimators': n,\n",
        "                    'rmse': m['rmse'],\n",
        "                    'mape': m['mape'],\n",
        "                    'r2': m['r2'],\n",
        "                })\n",
        "\n",
        "                exp.log_metrics({\n",
        "                    \"rmse\": float(m[\"rmse\"]),\n",
        "                    \"mape\": float(m[\"mape\"]),\n",
        "                    \"r2\": float(m[\"r2\"]),\n",
        "                })\n",
        "\n",
        "hp_df = pd.DataFrame(rows)\n",
        "# Drop any null metric rows to avoid seaborn errors\n",
        "hp_df = hp_df.dropna(subset=['mape', 'rmse']).sort_values('mape').reset_index(drop=True)\n",
        "\n",
        "# Train final model with best params (by RMSE) if available\n",
        "if not hp_df.empty:\n",
        "    best = hp_df.sort_values('rmse').iloc[0].to_dict()\n",
        "    xgb = XGBRegressor(\n",
        "        n_estimators=int(best['n_estimators']),\n",
        "        max_depth=int(best['max_depth']),\n",
        "        learning_rate=float(best['learning_rate']),\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        input_cols=feature_cols,\n",
        "        label_cols=[label_col],\n",
        "        output_cols=[output_col],\n",
        "        random_state=42,\n",
        "    )\n",
        "    pipe = Pipeline(steps=[('xgb', xgb)])\n",
        "    model = pipe.fit(train_df)\n",
        "    print('Model trained with best config (by RMSE):', best)\n",
        "else:\n",
        "    # Fall back to baseline params if sweep failed\n",
        "    best = {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.05}\n",
        "    xgb = XGBRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        input_cols=feature_cols,\n",
        "        label_cols=[label_col],\n",
        "        output_cols=[output_col],\n",
        "        random_state=42,\n",
        "    )\n",
        "    pipe = Pipeline(steps=[('xgb', xgb)])\n",
        "    model = pipe.fit(train_df)\n",
        "    print('Sweep produced no metrics; trained baseline config:', best)\n",
        "\n",
        "# Plot tuning curve similar to reference (learning_rate vs MAPE) if data available\n",
        "if not hp_df.empty:\n",
        "    sns.set_context('notebook', font_scale=0.9)\n",
        "    # Use FacetGrid-like relplot to match style\n",
        "    g = sns.relplot(\n",
        "        data=hp_df,\n",
        "        x='learning_rate', y='mape', hue='n_estimators', col='max_depth',\n",
        "        kind='line', marker='o', height=4, aspect=1.2\n",
        "    )\n",
        "    g.set_titles('max_depth = {col_name}')\n",
        "    g.set_axis_labels('learning_rate', 'MAPE')\n",
        "else:\n",
        "    print('Skipping tuning plot: no sweep results')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4b) (Disabled) Old sweep cell replaced by seaborn plot in cell 3\n",
        "# Left intentionally minimal to avoid duplicate tuning runs.\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Evaluate metrics (RMSE, MAPE, R2) using robust helper\n",
        "train_pred = model.predict(train_df).select(label_col, output_col)\n",
        "test_pred = model.predict(test_df).select(label_col, output_col)\n",
        "\n",
        "train_metrics = compute_metrics_quick(train_pred, label_col, output_col)\n",
        "test_metrics = compute_metrics_quick(test_pred, label_col, output_col)\n",
        "\n",
        "print({'train': train_metrics, 'test': test_metrics})\n",
        "\n",
        "# Log final evaluation and selection as a run for traceability\n",
        "with exp.start_run(\"final_best_selection\"):\n",
        "    exp.log_params({\n",
        "        \"selected_by\": \"rmse\",\n",
        "        \"best_max_depth\": int(best[\"max_depth\"]) if isinstance(best[\"max_depth\"], (int, float, str)) else best[\"max_depth\"],\n",
        "        \"best_learning_rate\": float(best[\"learning_rate\"]),\n",
        "        \"best_n_estimators\": int(best[\"n_estimators\"]),\n",
        "        \"feature_cols\": feature_cols,\n",
        "        \"label_col\": label_col,\n",
        "        \"output_col\": output_col,\n",
        "        \"time_split_cutoff\": cutoff.isoformat() if cutoff else \"auto\",\n",
        "        \"dataset_table\": \"PRICE_FEATURES\",\n",
        "        \"horizon_hours\": int(horizon_hours),\n",
        "    })\n",
        "\n",
        "    exp.log_metrics({\n",
        "        \"train_rmse\": float(train_metrics[\"rmse\"]),\n",
        "        \"train_mape\": float(train_metrics[\"mape\"]),\n",
        "        \"train_r2\": float(train_metrics[\"r2\"]),\n",
        "        \"test_rmse\": float(test_metrics[\"rmse\"]),\n",
        "        \"test_mape\": float(test_metrics[\"mape\"]),\n",
        "        \"test_r2\": float(test_metrics[\"r2\"]),\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Register in Model Registry (enable Snowflake explainability)\n",
        "reg = Registry(session=session, database_name='SP500_STOCK_DEMO', schema_name='DATA')\n",
        "model_name = 'XGB_SP500_RET3M'\n",
        "\n",
        "models_df = reg.show_models()\n",
        "if models_df.empty or model_name not in models_df['name'].to_list():\n",
        "    version = 'V_1'\n",
        "else:\n",
        "    import ast, builtins\n",
        "    max_v = builtins.max([int(v.split('_')[-1]) for v in ast.literal_eval(models_df.loc[models_df['name']==model_name,'versions'].values[0])])\n",
        "    version = f'V_{max_v+1}'\n",
        "\n",
        "# Provide background data to enable SHAP explainability in the Model Registry\n",
        "bg_pd = train_df.select(feature_cols).limit(1000).to_pandas()\n",
        "\n",
        "mv = reg.log_model(\n",
        "    model,\n",
        "    model_name=model_name,\n",
        "    version_name=version,\n",
        "    conda_dependencies=['snowflake-ml-python'],\n",
        "    comment='XGBRegressor predicting 3-month forward return from hourly features',\n",
        "    metrics={\n",
        "        'train_r2': train_metrics['r2'],\n",
        "        'test_r2': test_metrics['r2'],\n",
        "        'train_rmse': train_metrics['rmse'],\n",
        "        'test_rmse': test_metrics['rmse'],\n",
        "        'train_mape': train_metrics['mape'],\n",
        "        'test_mape': test_metrics['mape'],\n",
        "    },\n",
        "    sample_input_data=bg_pd,\n",
        "    options={'relax_version': False, 'enable_explainability': True}\n",
        ")\n",
        "reg.get_model(model_name).default = version\n",
        "\n",
        "# Log registry pointers into the final run context\n",
        "with exp.start_run(\"final_best_selection_regref\"):\n",
        "    exp.log_params({\n",
        "        \"registry_model_name\": model_name,\n",
        "        \"registry_version\": version,\n",
        "    })\n",
        "\n",
        "print({'model_name': model_name, 'version': version})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Snowflake explainability: compute SHAP-based global feature importance + seaborn bar plot\n",
        "# explain() availability was enabled during logging (cell 5) using background data.\n",
        "from snowflake.snowpark.functions import abs as sp_abs, col as sp_col\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reg = Registry(session=session, database_name='SP500_STOCK_DEMO', schema_name='DATA')\n",
        "mv_explain = reg.get_model('XGB_SP500_RET3M').default\n",
        "\n",
        "# Take a small sample of inputs (features only) for explanations\n",
        "sample_sp = ds.select(*feature_cols).limit(200)\n",
        "\n",
        "# Run SHAP explain through Model Registry (Snowflake-built explainability)\n",
        "explanations_sp = mv_explain.run(sample_sp, function_name='explain')\n",
        "\n",
        "# Aggregate mean absolute SHAP per feature\n",
        "ex_pd = explanations_sp.to_pandas()\n",
        "cols = list(ex_pd.columns)\n",
        "shap_map = {}\n",
        "for f in feature_cols:\n",
        "    if f in cols:\n",
        "        shap_map[f] = f\n",
        "    elif f\"SHAP_{f}\" in cols:\n",
        "        shap_map[f] = f\"SHAP_{f}\"\n",
        "\n",
        "imp_rows = []\n",
        "for f, c in shap_map.items():\n",
        "    imp = float(pd.Series(ex_pd[c]).abs().mean())\n",
        "    imp_rows.append({'feature': f, 'mean_abs_shap': imp})\n",
        "imp_df = pd.DataFrame(imp_rows).sort_values('mean_abs_shap', ascending=False)\n",
        "\n",
        "# Persist and plot\n",
        "session.create_dataframe(imp_df).write.save_as_table('FEATURE_SHAP_GLOBAL_TOP', mode='overwrite')\n",
        "\n",
        "sns.set_context('notebook', font_scale=0.9)\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(data=imp_df, x='mean_abs_shap', y='feature', orient='h')\n",
        "plt.title('Global feature importance (mean |SHAP|)')\n",
        "plt.xlabel('Mean |SHAP|')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(imp_df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6b) Explainability: feature importances (from sklearn view) — optional quick view\n",
        "sk_pipe = model.to_sklearn()\n",
        "# Find xgb step\n",
        "xgb_step = None\n",
        "for name, step in sk_pipe.named_steps.items():\n",
        "    if 'xgb' in name.lower():\n",
        "        xgb_step = step\n",
        "        break\n",
        "\n",
        "if xgb_step is not None and hasattr(xgb_step, 'feature_importances_'):\n",
        "    import pandas as pd\n",
        "    fi = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': xgb_step.feature_importances_.tolist()[:len(feature_cols)]\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    fi_sp = session.create_dataframe(fi)\n",
        "    fi_sp.write.save_as_table('FEATURE_IMPORTANCE_SP500', mode='overwrite')\n",
        "    print('Saved FEATURE_IMPORTANCE_SP500')\n",
        "else:\n",
        "    print('Feature importances not available from sklearn object.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) Optional: List recent runs from Experiment Tracking (for live demo)\n",
        "print(\"Experiment:\", \"SP500_XGB_RET3M\")\n",
        "\n",
        "runs = None\n",
        "used_method = None\n",
        "\n",
        "candidates = [\n",
        "    (\"show_runs(limit=20)\", lambda: exp.show_runs(limit=20)),\n",
        "    (\"list_runs(limit=20)\", lambda: exp.list_runs(limit=20)),\n",
        "    (\"search_runs(limit=20)\", lambda: exp.search_runs(limit=20)),\n",
        "    (\"get_runs(limit=20)\", lambda: exp.get_runs(limit=20)),\n",
        "    (\"show_runs()\", lambda: exp.show_runs()),\n",
        "    (\"list_runs()\", lambda: exp.list_runs()),\n",
        "    (\"search_runs()\", lambda: exp.search_runs()),\n",
        "    (\"get_runs()\", lambda: exp.get_runs()),\n",
        "]\n",
        "\n",
        "for label, fn in candidates:\n",
        "    try:\n",
        "        runs = fn()\n",
        "        used_method = label\n",
        "        break\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if runs is not None:\n",
        "    print(f\"Listed runs via: {used_method}\")\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        from IPython.display import display\n",
        "        display(runs)\n",
        "    except Exception:\n",
        "        print(runs)\n",
        "else:\n",
        "    print(\"Could not list runs programmatically. Open Snowsight > ML > Experiments and select 'SP500_XGB_RET3M'.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
