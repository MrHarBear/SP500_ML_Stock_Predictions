{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SP500 Stock Demo â€” Notebook 01: Data Prep\n",
        "\n",
        "- Connect to Snowflake and set context\n",
        "- Subset `CORTEX_DEMO.FSI_STOCKS_INSIGHT.DAILY_STOCK_PRICE` to S&P 500 tickers\n",
        "- Simulate hourly OHLCV from daily for a small demo slice\n",
        "- Persist curated tables into `SP500_STOCK_DEMO.DATA`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0) Imports and session\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark.functions import col, lit, to_timestamp, dateadd\n",
        "from snowflake.snowpark.types import StructType, StructField, StringType, TimestampType, FloatType, IntegerType\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "session = get_active_session()\n",
        "\n",
        "# Print context\n",
        "env = session.sql('select current_user(), current_role(), current_database(), current_schema(), current_warehouse(), current_version()').collect()[0]\n",
        "print({'user': env[0], 'role': env[1], 'db': env[2], 'schema': env[3], 'wh': env[4], 'version': env[5]})\n",
        "\n",
        "# Set target DB/Schema\n",
        "session.sql(\"USE DATABASE SP500_STOCK_DEMO\").collect()\n",
        "session.sql(\"USE SCHEMA DATA\").collect()\n",
        "session.sql(\"USE WAREHOUSE DEMO_WH_M\").collect()\n",
        "\n",
        "# Parameter: number of years of data to include (set to 1 for fast local testing; change to 7 when ready)\n",
        "YEARS_BACK = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Load S&P 500 tickers\n",
        "# Option A: use a small inline list for demo\n",
        "# sp500_tickers = pd.DataFrame({'TICKER': [\n",
        "#     'AAPL','MSFT','AMZN','GOOGL','META','NVDA','BRK.B','JPM','JNJ','XOM'\n",
        "# ]})\n",
        "# sp500_df = session.create_dataframe(sp500_tickers)\n",
        "# sp500_df.write.save_as_table('SP500_TICKERS', mode='overwrite')\n",
        "\n",
        "# Option B: if you have a maintained table, reference it directly instead of writing above\n",
        "sp500_df = session.table('SP_500_LIST')\n",
        "sp500_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Subset daily source to S&P 500 (standardize to TICKER/DATE)\n",
        "source_table = 'CORTEX_DEMO.FSI_STOCKS_INSIGHT.DAILY_STOCK_PRICE'\n",
        "\n",
        "daily = session.table(source_table)\n",
        "subset = (\n",
        "    daily.join(sp500_df, on=(daily['TICKER'] == sp500_df['SYMBOL']))\n",
        "         .select(\n",
        "             daily['TICKER'].alias('TICKER'),\n",
        "             daily['DATE'].alias('DATE'),\n",
        "             daily['OPEN'],\n",
        "             daily['HIGH'],\n",
        "             daily['LOW'],\n",
        "             daily['CLOSE'],\n",
        "             daily['VOLUME']\n",
        "         )\n",
        ")\n",
        "subset.limit(5).show()\n",
        "subset.write.save_as_table('DAILY_SP500', mode='overwrite')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wh = str(session.get_current_warehouse()).strip('\"')\n",
        "print(f\"Current warehouse: {wh}\")\n",
        "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n",
        "\n",
        "session.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n",
        "\n",
        "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Simulate hourly OHLCV from daily (TICKER/DATE standard)\n",
        "# Brownian-bridge-like noise; 6 hours between 10:00 and 16:00 local\n",
        "\n",
        "from snowflake.snowpark.functions import to_timestamp_ltz, current_date, lit\n",
        "\n",
        "# Window for testing is parameterized by YEARS_BACK (set to 1 by default)\n",
        "cutoff = session.sql(f\"select dateadd('year', -{YEARS_BACK}, current_date()) as D\").collect()[0]['D']\n",
        "limited = session.table('DAILY_SP500').filter(col('DATE') >= lit(cutoff))\n",
        "\n",
        "pdf = limited.to_pandas()\n",
        "pdf = pdf.sort_values(['TICKER','DATE'])\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "rows = []\n",
        "for sym, grp in pdf.groupby('TICKER'):\n",
        "    for _, r in grp.iterrows():\n",
        "        o, h, l, c = float(r.OPEN), float(r.HIGH), float(r.LOW), float(r.CLOSE)\n",
        "        v = float(r.VOLUME)\n",
        "        date = pd.to_datetime(r.DATE)\n",
        "        hours = pd.date_range(date + pd.Timedelta(hours=10), date + pd.Timedelta(hours=16), freq='1h', inclusive='left')\n",
        "        # Distribute total volume\n",
        "        vol_alloc = rng.multinomial(int(v) if v>0 else 0, np.ones(len(hours))/len(hours)) if v>0 else np.zeros(len(hours), dtype=int)\n",
        "        # Brownian bridge around open->close bounded by daily high/low\n",
        "        steps = len(hours)\n",
        "        noise = rng.normal(0, 1, steps)\n",
        "        noise = (noise - noise.mean()) / (noise.std() + 1e-6)\n",
        "        path = o + (c - o) * (np.arange(steps)/(steps-1 if steps>1 else 1)) + noise * max((h-l)/6, 1e-6)\n",
        "        path = np.clip(path, l, h)\n",
        "        # Create synthetic OHLC intraday using small windows around path\n",
        "        for i, ts in enumerate(hours):\n",
        "            base = float(path[i])\n",
        "            hi = float(min(h, base + abs(base)*0.002))\n",
        "            lo = float(max(l, base - abs(base)*0.002))\n",
        "            op = float(base)\n",
        "            cl = float(base + rng.normal(0, abs(base)*0.0008))\n",
        "            rows.append([sym, ts, op, hi, lo, cl, int(vol_alloc[i])])\n",
        "\n",
        "hourly_pdf = pd.DataFrame(rows, columns=['TICKER','TS','OPEN','HIGH','LOW','CLOSE','VOLUME'])\n",
        "\n",
        "hourly_df = session.create_dataframe(hourly_pdf)\n",
        "hourly_df = hourly_df.with_column('TS', to_timestamp(col('TS')))\n",
        "\n",
        "hourly_df.write.save_as_table('HOURLY_SP500_SIM', mode='overwrite')\n",
        "\n",
        "hourly_df.limit(5).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3b) Optional: Batched hourly generation (memory-safe) into a separate table\n",
        "# - Keeps the original full-table flow intact\n",
        "# - Generates intraday hours for small ticker chunks and writes to HOURLY_SP500_SIM_BATCH\n",
        "# - Set TEST_FIRST_BATCH=True to only process the first chunk for quick validation\n",
        "\n",
        "from snowflake.snowpark.functions import col as sp_col, lit as sp_lit\n",
        "\n",
        "BATCH_TABLE = 'HOURLY_SP500_SIM_BATCH'\n",
        "BATCH_SIZE = 10           # number of tickers per chunk\n",
        "TEST_FIRST_BATCH = True   # set False to process all chunks\n",
        "\n",
        "# Build list of tickers from SP_500_LIST (SYMBOL column)\n",
        "all_symbols = [r['SYMBOL'] for r in sp500_df.select('SYMBOL').collect()]\n",
        "\n",
        "# Helper to chunk list\n",
        "def chunk_list(items, size):\n",
        "    for i in range(0, len(items), size):\n",
        "        yield items[i:i+size]\n",
        "\n",
        "# Ensure cutoff exists from previous cell; if not, compute here as 1-year default\n",
        "try:\n",
        "    _ = cutoff\n",
        "except NameError:\n",
        "    cutoff = session.sql(\"select dateadd('year', -1, current_date()) as D\").collect()[0]['D']\n",
        "\n",
        "mode = 'overwrite'\n",
        "chunks = list(chunk_list(all_symbols, BATCH_SIZE))\n",
        "print({'chunks': len(chunks), 'batch_size': BATCH_SIZE, 'test_first_batch': TEST_FIRST_BATCH})\n",
        "\n",
        "for idx, symbols in enumerate(chunks):\n",
        "    # Filter daily subset to batch tickers and cutoff window\n",
        "    limited_chunk = (\n",
        "        session.table('DAILY_SP500')\n",
        "               .filter((sp_col('DATE') >= sp_lit(cutoff)) & (sp_col('TICKER').isin(symbols)))\n",
        "    )\n",
        "    pdf = limited_chunk.to_pandas().sort_values(['TICKER','DATE'])\n",
        "\n",
        "    # Brownian-bridge-like intraday synthesis\n",
        "    rng = np.random.default_rng(42 + idx)\n",
        "    rows = []\n",
        "    for sym, grp in pdf.groupby('TICKER'):\n",
        "        for _, r in grp.iterrows():\n",
        "            o, h, l, c = float(r.OPEN), float(r.HIGH), float(r.LOW), float(r.CLOSE)\n",
        "            v = float(r.VOLUME)\n",
        "            date = pd.to_datetime(r.DATE)\n",
        "            hours = pd.date_range(date + pd.Timedelta(hours=10), date + pd.Timedelta(hours=16), freq='1h', inclusive='left')\n",
        "            vol_alloc = rng.multinomial(int(v) if v>0 else 0, np.ones(len(hours))/len(hours)) if v>0 else np.zeros(len(hours), dtype=int)\n",
        "            steps = len(hours)\n",
        "            noise = rng.normal(0, 1, steps)\n",
        "            noise = (noise - noise.mean()) / (noise.std() + 1e-6)\n",
        "            path = o + (c - o) * (np.arange(steps)/(steps-1 if steps>1 else 1)) + noise * max((h-l)/6, 1e-6)\n",
        "            path = np.clip(path, l, h)\n",
        "            for i, ts in enumerate(hours):\n",
        "                base = float(path[i])\n",
        "                hi = float(min(h, base + abs(base)*0.002))\n",
        "                lo = float(max(l, base - abs(base)*0.002))\n",
        "                op = float(base)\n",
        "                cl = float(base + rng.normal(0, abs(base)*0.0008))\n",
        "                rows.append([sym, ts, op, hi, lo, cl, int(vol_alloc[i])])\n",
        "\n",
        "    if not rows:\n",
        "        print(f'Chunk {idx+1}/{len(chunks)}: no rows (skipped)')\n",
        "    else:\n",
        "        hourly_pdf = pd.DataFrame(rows, columns=['TICKER','TS','OPEN','HIGH','LOW','CLOSE','VOLUME'])\n",
        "        hourly_df = session.create_dataframe(hourly_pdf).with_column('TS', to_timestamp(col('TS')))\n",
        "        hourly_df.write.save_as_table(BATCH_TABLE, mode=mode)\n",
        "        print(f'Chunk {idx+1}/{len(chunks)}: wrote {len(rows)} rows in mode={mode}')\n",
        "        mode = 'append'  # subsequent chunks append\n",
        "\n",
        "    if TEST_FIRST_BATCH:\n",
        "        break\n",
        "\n",
        "# Preview and counts for the batch table\n",
        "try:\n",
        "    session.table(BATCH_TABLE).limit(5).show()\n",
        "    print({'batch_rows': session.table(BATCH_TABLE).count()})\n",
        "except Exception as e:\n",
        "    print('Batch table preview unavailable:', e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Indexes for downstream feature generation\n",
        "session.sql(\"CREATE OR REPLACE VIEW HOURLY_SP500_SIM_VIEW AS \\\n",
        "    SELECT TICKER, TS, OPEN, HIGH, LOW, CLOSE, VOLUME FROM HOURLY_SP500_SIM\").collect()\n",
        "\n",
        "print('Created tables: DAILY_SP500, HOURLY_SP500_SIM and view HOURLY_SP500_SIM_VIEW')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wh = str(session.get_current_warehouse()).strip('\"')\n",
        "print(f\"Current warehouse: {wh}\")\n",
        "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n",
        "\n",
        "session.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = SMALL WAIT_FOR_COMPLETION = TRUE\").collect()\n",
        "\n",
        "print(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
